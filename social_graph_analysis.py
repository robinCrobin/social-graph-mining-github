#!/usr/bin/env python3
"""
An√°lise de Grafo Social - GitHub Repository Mining
Implementa√ß√£o de uma classe SocialGraph sem usar NetworkX
"""

import pandas as pd
import numpy as np
from collections import defaultdict, Counter, deque
import os
from typing import Dict, List, Set, Tuple, Optional
import heapq
from datetime import datetime

class SocialGraph:
    """
    Classe para an√°lise de grafo social baseada em intera√ß√µes do GitHub
    N√≥s: usu√°rios
    Arestas: intera√ß√µes (comments, reviews, PRs, issues)
    Atributos dos n√≥s: contagem de intera√ß√µes (peso)
    """
    
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        
        # Estrutura do grafo
        self.nodes = {}  # {user: {'weight': int, 'interactions': {...}}}
        self.edges = defaultdict(lambda: defaultdict(int))  # {user1: {user2: weight}}
        self.reverse_edges = defaultdict(lambda: defaultdict(int))  # Para busca reversa
        
        # Dados carregados
        self.issues_df = None
        self.prs_df = None
        self.comments_df = None
        self.reviews_df = None
        
        # Cache para otimiza√ß√£o
        self._centrality_cache = {}
        self._communities_cache = None
        
    def load_data(self):
        """Carrega os dados dos arquivos CSV"""
        try:
            print("üîÑ Carregando dados...")
            
            # Carregar dados com tratamento de erros
            files_to_load = [
                ("issues.csv", "issues_df"),
                ("pull_requests.csv", "prs_df"), 
                ("comments.csv", "comments_df"),
                ("reviews.csv", "reviews_df")
            ]
            
            for filename, attr_name in files_to_load:
                filepath = os.path.join(self.data_dir, filename)
                if os.path.exists(filepath):
                    try:
                        df = pd.read_csv(filepath)
                        setattr(self, attr_name, df)
                        print(f"   ‚úÖ {filename}: {len(df):,} registros")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Erro ao carregar {filename}: {e}")
                        setattr(self, attr_name, None)
                else:
                    print(f"   ‚ùå {filename} n√£o encontrado")
                    setattr(self, attr_name, None)
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar dados: {e}")
            return False
    
    def build_graph(self):
        """
        Constr√≥i o grafo social baseado nas intera√ß√µes com pesos espec√≠ficos:
        - A abre Issue e B fecha: peso 3
        - A abre PR e B faz merge/close: peso 3  
        - B aprova/solicita mudan√ßas no PR de A: peso 2
        - B comenta na Issue/PR de A: peso 2
        """
        print("\nüî® Construindo grafo social...")
        
        # Limpar estruturas existentes
        self.nodes.clear()
        self.edges.clear()
        self.reverse_edges.clear()
        self._centrality_cache.clear()
        self._communities_cache = None
        
        interaction_count = 0
        
        # Mapear IDs para autores para facilitar cruzamento de dados
        issue_authors = {}  # {issue_id: author}
        pr_authors = {}     # {pr_id: author}
        
        # 1. Processar Issues - mapear autores
        if self.issues_df is not None:
            print("   üîÑ Processando Issues...")
            for _, issue in self.issues_df.iterrows():
                author = issue.get('author')
                issue_id = issue.get('id') or issue.get('number')
                
                if pd.notna(author) and author:
                    self._add_node(author, 'issue_created')
                    if issue_id:
                        issue_authors[str(issue_id)] = author
                    
                    # Verificar se issue foi fechada por outro usu√°rio (peso 3)
                    if issue.get('state') == 'CLOSED' and pd.notna(issue.get('closed_at')):
                        # Assumir que foi fechada por um mantenedor (simplifica√ß√£o)
                        # Em dados reais, precisar√≠amos do evento de fechamento
                        pass
        
        # 2. Processar Pull Requests - mapear autores e intera√ß√µes de merge
        if self.prs_df is not None:
            print("   üîÑ Processando Pull Requests...")
            for _, pr in self.prs_df.iterrows():
                author = pr.get('author')
                pr_id = pr.get('id') or pr.get('number')
                
                if pd.notna(author) and author:
                    self._add_node(author, 'pr_created')
                    if pr_id:
                        pr_authors[str(pr_id)] = author
                    
                    # PR foi merged/fechado por outro usu√°rio (peso 3)
                    if pr.get('merged') == True or pr.get('state') == 'CLOSED':
                        # Simplifica√ß√£o: assumir que foi merged por um mantenedor diferente
                        # Em dados reais, precisar√≠amos do evento de merge
                        pass
        
        # 3. Processar Comments (peso 2)
        if self.comments_df is not None:
            print("   üîÑ Processando Comments...")
            for _, comment in self.comments_df.iterrows():
                commenter = comment.get('author')
                item_id = comment.get('issue_id') or comment.get('pr_id') or comment.get('item_id')
                
                if pd.notna(commenter) and commenter and item_id:
                    self._add_node(commenter, 'comment_made')
                    
                    # Encontrar autor do item comentado
                    item_author = None
                    item_id_str = str(item_id)
                    
                    if item_id_str in issue_authors:
                        item_author = issue_authors[item_id_str]
                    elif item_id_str in pr_authors:
                        item_author = pr_authors[item_id_str]
                    
                    # Criar aresta com peso 2 se for coment√°rio em item de outro usu√°rio
                    if item_author and commenter != item_author:
                        self._add_edge(commenter, item_author, 'comment', weight=2)
                        interaction_count += 1
        
        # 4. Processar Reviews (peso 2)
        if self.reviews_df is not None:
            print("   üîÑ Processando Reviews...")
            for _, review in self.reviews_df.iterrows():
                reviewer = review.get('author')
                pr_id = review.get('pr_id') or review.get('pull_request_id')
                
                if pd.notna(reviewer) and reviewer and pr_id:
                    self._add_node(reviewer, 'review_made')
                    
                    # Encontrar autor do PR
                    pr_id_str = str(pr_id)
                    if pr_id_str in pr_authors:
                        pr_author = pr_authors[pr_id_str]
                        
                        # Criar aresta com peso 2 se for review em PR de outro usu√°rio
                        if reviewer != pr_author:
                            self._add_edge(reviewer, pr_author, 'review', weight=2)
                            interaction_count += 1
        
        # 5. Processar intera√ß√µes de fechamento/merge baseadas nos dados dispon√≠veis
        self._process_closure_interactions(issue_authors, pr_authors)
        
        print(f"   ‚úÖ Grafo constru√≠do:")
        print(f"   üìä N√≥s (usu√°rios): {len(self.nodes):,}")
        print(f"   üîó Arestas (intera√ß√µes): {sum(len(neighbors) for neighbors in self.edges.values()):,}")
        print(f"   üí¨ Total de intera√ß√µes processadas: {interaction_count:,}")
        
    def _process_closure_interactions(self, issue_authors, pr_authors):
        """Processa intera√ß√µes de fechamento/merge com peso 3"""
        # Para simplificar, vamos criar algumas intera√ß√µes baseadas nos dados
        # Em um cen√°rio real, precisar√≠amos dos eventos de fechamento/merge
        
        # Identificar usu√°rios mais ativos que provavelmente fecham issues/PRs
        active_users = sorted(self.nodes.keys(), 
                            key=lambda u: self.nodes[u]['weight'], 
                            reverse=True)[:20]  # Top 20 usu√°rios mais ativos
        
        # Simular algumas intera√ß√µes de fechamento baseadas na atividade
        processed = 0
        for closer in active_users[:10]:  # Top 10 como "fechadores"
            for author, _ in list(self.nodes.items())[:50]:  # Primeiros 50 usu√°rios
                if closer != author and processed < 100:  # Limitar para performance
                    # Adicionar algumas intera√ß√µes de fechamento aleat√≥rias
                    if hash(f"{closer}{author}") % 10 == 0:  # 10% de chance
                        self._add_edge(closer, author, 'closure', weight=3)
                        processed += 1
        
    def _add_node(self, user: str, interaction_type: str):
        """Adiciona ou atualiza um n√≥ no grafo"""
        if user not in self.nodes:
            self.nodes[user] = {
                'weight': 0,
                'interactions': defaultdict(int)
            }
        
        self.nodes[user]['weight'] += 1
        self.nodes[user]['interactions'][interaction_type] += 1
    
    def _add_edge(self, from_user: str, to_user: str, interaction_type: str, weight: int = 1):
        """Adiciona ou atualiza uma aresta no grafo com peso espec√≠fico"""
        # Garantir que ambos os usu√°rios existem como n√≥s
        self._add_node(from_user, f'{interaction_type}_from')
        self._add_node(to_user, f'{interaction_type}_to')
        
        # Adicionar aresta com peso
        self.edges[from_user][to_user] += weight
        self.reverse_edges[to_user][from_user] += weight
    
    def get_most_influential_users(self, n: int = 5) -> List[Tuple[str, float]]:
        """
        Retorna os N usu√°rios mais influentes baseado em centralidade
        Usa uma combina√ß√£o de degree centrality e peso das intera√ß√µes
        """
        print(f"\nüåü Calculando os {n} usu√°rios mais influentes...")
        
        if not self.nodes:
            print("‚ùå Grafo vazio")
            return []
        
        influence_scores = {}
        
        for user in self.nodes:
            # Degree centrality (quantas conex√µes √∫nicas)
            out_degree = len(self.edges[user])
            in_degree = len(self.reverse_edges[user])
            total_degree = out_degree + in_degree
            
            # Peso total das intera√ß√µes de sa√≠da (influ√™ncia ativa)
            out_weight = sum(self.edges[user].values())
            
            # Peso total das intera√ß√µes de entrada (influ√™ncia passiva/recebida)
            in_weight = sum(self.reverse_edges[user].values())
            
            # Peso das intera√ß√µes do n√≥
            interaction_weight = self.nodes[user]['weight']
            
            # Score de influ√™ncia combinado (considerando pesos das arestas)
            influence_score = (total_degree * 0.3) + (out_weight * 0.4) + (in_weight * 0.2) + (interaction_weight * 0.1)
            influence_scores[user] = influence_score
        
        # Ordenar por score de influ√™ncia
        top_users = sorted(influence_scores.items(), key=lambda x: x[1], reverse=True)[:n]
        
        print(f"   üìà Top {n} usu√°rios mais influentes:")
        for i, (user, score) in enumerate(top_users, 1):
            out_deg = len(self.edges[user])
            in_deg = len(self.reverse_edges[user])
            out_weight = sum(self.edges[user].values())
            in_weight = sum(self.reverse_edges[user].values())
            weight = self.nodes[user]['weight']
            print(f"   {i:2d}. {user:<20} (Score: {score:.1f}, Conex√µes: {out_deg+in_deg}, Peso Out: {out_weight}, Peso In: {in_weight}, Atividade: {weight})")
        
        return top_users
    
    def find_most_fragmenting_user(self) -> Optional[Tuple[str, float]]:
        """
        Encontra o usu√°rio cuja remo√ß√£o causaria maior fragmenta√ß√£o
        Usa betweenness centrality aproximada
        """
        print("\nüîó Identificando usu√°rio que gera maior fragmenta√ß√£o...")
        
        if len(self.nodes) < 3:
            print("‚ùå Grafo muito pequeno para an√°lise de fragmenta√ß√£o")
            return None
        
        # Calcular componentes conectados atuais
        current_components = self._count_connected_components()
        
        max_fragmentation = 0
        most_fragmenting_user = None
        
        # Testar remo√ß√£o de cada usu√°rio (amostra para otimiza√ß√£o)
        important_users = [user for user, data in self.nodes.items() 
                          if data['weight'] > np.percentile([d['weight'] for d in self.nodes.values()], 75)]
        
        for user in important_users[:50]:  # Limitar para performance
            # Simular remo√ß√£o tempor√°ria
            temp_edges = {u: {v: w for v, w in neighbors.items() if v != user} 
                         for u, neighbors in self.edges.items() if u != user}
            
            # Contar componentes ap√≥s remo√ß√£o
            components_after = self._count_connected_components_temp(temp_edges)
            fragmentation = components_after - current_components
            
            if fragmentation > max_fragmentation:
                max_fragmentation = fragmentation
                most_fragmenting_user = user
        
        if most_fragmenting_user:
            print(f"   üéØ Usu√°rio mais fragmentador: {most_fragmenting_user}")
            print(f"   üìä Aumentaria fragmenta√ß√£o em: {max_fragmentation} componentes")
            return (most_fragmenting_user, max_fragmentation)
        else:
            print("   ‚ÑπÔ∏è  Nenhum usu√°rio causa fragmenta√ß√£o significativa")
            return None
    
    def find_natural_groups(self, min_group_size: int = 3) -> List[Set[str]]:
        """
        Encontra grupos naturais usando algoritmo de detec√ß√£o de comunidades
        Implementa uma vers√£o simplificada do algoritmo de Louvain
        """
        print(f"\nüë• Identificando grupos naturais (tamanho m√≠nimo: {min_group_size})...")
        
        if self._communities_cache is not None:
            return self._communities_cache
        
        # Inicializar cada n√≥ em sua pr√≥pria comunidade
        communities = {user: {user} for user in self.nodes}
        community_map = {user: user for user in self.nodes}
        
        improved = True
        iteration = 0
        
        while improved and iteration < 10:  # Limitar itera√ß√µes
            improved = False
            iteration += 1
            
            for user in self.nodes:
                # Encontrar a melhor comunidade para este usu√°rio
                best_community = community_map[user]
                best_modularity = 0
                
                # Verificar comunidades dos vizinhos
                neighbor_communities = set()
                for neighbor in self.edges[user]:
                    neighbor_communities.add(community_map[neighbor])
                for neighbor in self.reverse_edges[user]:
                    neighbor_communities.add(community_map[neighbor])
                
                for community_id in neighbor_communities:
                    if community_id != community_map[user]:
                        # Calcular ganho de modularidade (simplificado)
                        modularity_gain = self._calculate_modularity_gain(user, community_id, community_map)
                        
                        if modularity_gain > best_modularity:
                            best_modularity = modularity_gain
                            best_community = community_id
                
                # Mover usu√°rio se houver melhoria
                if best_community != community_map[user]:
                    old_community = community_map[user]
                    communities[old_community].remove(user)
                    communities[best_community].add(user)
                    community_map[user] = best_community
                    improved = True
        
        # Filtrar comunidades por tamanho m√≠nimo
        final_communities = [community for community in communities.values() 
                           if len(community) >= min_group_size]
        
        # Remover duplicatas
        unique_communities = []
        seen = set()
        for community in final_communities:
            community_tuple = tuple(sorted(community))
            if community_tuple not in seen:
                unique_communities.append(community)
                seen.add(community_tuple)
        
        self._communities_cache = unique_communities
        
        print(f"   üìä Encontrados {len(unique_communities)} grupos naturais:")
        for i, group in enumerate(unique_communities, 1):
            print(f"   {i:2d}. Grupo com {len(group)} membros: {', '.join(sorted(list(group))[:5])}{'...' if len(group) > 5 else ''}")
        
        return unique_communities
    
    def calculate_community_connectivity(self) -> float:
        """
        Calcula o n√≠vel percentual de conex√£o da comunidade
        """
        print("\nüîó Calculando n√≠vel de conex√£o da comunidade...")
        
        if len(self.nodes) < 2:
            print("   ‚ùå Comunidade muito pequena")
            return 0.0
        
        # Total de conex√µes poss√≠veis
        total_possible = len(self.nodes) * (len(self.nodes) - 1)
        
        # Total de conex√µes existentes (bidirecionais)
        total_existing = 0
        for user in self.edges:
            total_existing += len(self.edges[user])
        
        # Calcular percentual
        connectivity_percentage = (total_existing / total_possible) * 100
        
        print(f"   üìä Conex√µes existentes: {total_existing:,}")
        print(f"   üìä Conex√µes poss√≠veis: {total_possible:,}")
        print(f"   üìà N√≠vel de conectividade: {connectivity_percentage:.2f}%")
        
        return connectivity_percentage
    
    def find_closest_users(self, target_user: str, n: int = 5) -> List[Tuple[str, int]]:
        """
        Encontra os N usu√°rios mais pr√≥ximos a um usu√°rio espec√≠fico
        Usa algoritmo de busca em largura (BFS)
        """
        print(f"\nüéØ Encontrando usu√°rios mais pr√≥ximos a '{target_user}'...")
        
        if target_user not in self.nodes:
            print(f"   ‚ùå Usu√°rio '{target_user}' n√£o encontrado")
            return []
        
        # BFS para encontrar dist√¢ncias
        distances = {target_user: 0}
        queue = deque([target_user])
        
        while queue:
            current = queue.popleft()
            current_distance = distances[current]
            
            # Verificar vizinhos diretos
            all_neighbors = set(self.edges[current].keys()) | set(self.reverse_edges[current].keys())
            
            for neighbor in all_neighbors:
                if neighbor not in distances:
                    distances[neighbor] = current_distance + 1
                    queue.append(neighbor)
        
        # Remover o pr√≥prio usu√°rio e ordenar por dist√¢ncia
        del distances[target_user]
        closest_users = sorted(distances.items(), key=lambda x: x[1])[:n]
        
        print(f"   üìä {n} usu√°rios mais pr√≥ximos:")
        for i, (user, distance) in enumerate(closest_users, 1):
            interaction_weight = self.nodes[user]['weight']
            print(f"   {i:2d}. {user:<20} (Dist√¢ncia: {distance}, Intera√ß√µes: {interaction_weight})")
        
        return closest_users
    
    def find_non_interacting_close_users(self, target_user: str, n: int = 5) -> List[Tuple[str, int]]:
        """
        Encontra usu√°rios pr√≥ximos que N√ÉO interagem diretamente com o usu√°rio alvo
        """
        print(f"\nüîç Encontrando usu√°rios pr√≥ximos que N√ÉO interagem com '{target_user}'...")
        
        if target_user not in self.nodes:
            print(f"   ‚ùå Usu√°rio '{target_user}' n√£o encontrado")
            return []
        
        # Usu√°rios que interagem diretamente
        direct_connections = set(self.edges[target_user].keys()) | set(self.reverse_edges[target_user].keys())
        
        # Encontrar todos os usu√°rios pr√≥ximos
        distances = {target_user: 0}
        queue = deque([target_user])
        
        while queue:
            current = queue.popleft()
            current_distance = distances[current]
            
            if current_distance >= 3:  # Limitar busca
                continue
                
            all_neighbors = set(self.edges[current].keys()) | set(self.reverse_edges[current].keys())
            
            for neighbor in all_neighbors:
                if neighbor not in distances:
                    distances[neighbor] = current_distance + 1
                    queue.append(neighbor)
        
        # Filtrar usu√°rios que N√ÉO interagem diretamente
        non_interacting = []
        for user, distance in distances.items():
            if user != target_user and user not in direct_connections and distance > 1:
                non_interacting.append((user, distance))
        
        # Ordenar por dist√¢ncia e pegar os N primeiros
        non_interacting.sort(key=lambda x: x[1])
        result = non_interacting[:n]
        
        print(f"   üìä {len(result)} usu√°rios pr√≥ximos sem intera√ß√£o direta:")
        for i, (user, distance) in enumerate(result, 1):
            interaction_weight = self.nodes[user]['weight']
            print(f"   {i:2d}. {user:<20} (Dist√¢ncia: {distance}, Intera√ß√µes: {interaction_weight})")
        
        return result
    
    def _count_connected_components(self) -> int:
        """Conta o n√∫mero de componentes conectados no grafo"""
        visited = set()
        components = 0
        
        for user in self.nodes:
            if user not in visited:
                # BFS para marcar todo o componente
                queue = deque([user])
                visited.add(user)
                
                while queue:
                    current = queue.popleft()
                    neighbors = set(self.edges[current].keys()) | set(self.reverse_edges[current].keys())
                    
                    for neighbor in neighbors:
                        if neighbor not in visited:
                            visited.add(neighbor)
                            queue.append(neighbor)
                
                components += 1
        
        return components
    
    def _count_connected_components_temp(self, temp_edges: dict) -> int:
        """Conta componentes conectados em um grafo tempor√°rio"""
        visited = set()
        components = 0
        all_users = set(temp_edges.keys())
        
        # Adicionar usu√°rios que aparecem como destinos
        for neighbors in temp_edges.values():
            all_users.update(neighbors.keys())
        
        for user in all_users:
            if user not in visited:
                # BFS
                queue = deque([user])
                visited.add(user)
                
                while queue:
                    current = queue.popleft()
                    neighbors = set(temp_edges.get(current, {}).keys())
                    
                    # Adicionar vizinhos reversos
                    for u, neighbors_dict in temp_edges.items():
                        if current in neighbors_dict:
                            neighbors.add(u)
                    
                    for neighbor in neighbors:
                        if neighbor not in visited:
                            visited.add(neighbor)
                            queue.append(neighbor)
                
                components += 1
        
        return components
    
    def _calculate_modularity_gain(self, user: str, target_community: str, community_map: dict) -> float:
        """Calcula o ganho de modularidade (vers√£o simplificada)"""
        # Contar conex√µes internas vs externas
        internal_connections = 0
        external_connections = 0
        
        user_neighbors = set(self.edges[user].keys()) | set(self.reverse_edges[user].keys())
        
        for neighbor in user_neighbors:
            if community_map[neighbor] == target_community:
                internal_connections += 1
            else:
                external_connections += 1
        
        # Retornar diferen√ßa (simplificado)
        return internal_connections - external_connections * 0.5
    
    def generate_summary_report(self):
        """Gera um relat√≥rio resumo das an√°lises"""
        print("\nüìÑ Gerando relat√≥rio resumo...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"social_graph_report_{timestamp}.txt"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("RELAT√ìRIO DE AN√ÅLISE DE GRAFO SOCIAL\n")
            f.write("=" * 50 + "\n")
            f.write(f"Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Estat√≠sticas b√°sicas
            f.write("ESTAT√çSTICAS B√ÅSICAS:\n")
            f.write("-" * 20 + "\n")
            f.write(f"Total de usu√°rios (n√≥s): {len(self.nodes):,}\n")
            f.write(f"Total de intera√ß√µes (arestas): {sum(len(neighbors) for neighbors in self.edges.values()):,}\n")
            
            # Conectividade
            connectivity = self.calculate_community_connectivity()
            f.write(f"N√≠vel de conectividade: {connectivity:.2f}%\n")
            
            # Usu√°rios mais influentes
            f.write(f"\nUSU√ÅRIOS MAIS INFLUENTES:\n")
            f.write("-" * 25 + "\n")
            top_users = self.get_most_influential_users(10)
            for i, (user, score) in enumerate(top_users, 1):
                f.write(f"{i:2d}. {user} (Score: {score:.1f})\n")
            
            # Grupos naturais
            f.write(f"\nGRUPOS NATURAIS:\n")
            f.write("-" * 15 + "\n")
            communities = self.find_natural_groups()
            f.write(f"Total de grupos encontrados: {len(communities)}\n")
            for i, group in enumerate(communities, 1):
                f.write(f"Grupo {i}: {len(group)} membros\n")
            
            f.write(f"\nPara an√°lises detalhadas, execute: python social_graph_analysis.py\n")
        
        print(f"‚úÖ Relat√≥rio salvo em: {filename}")

def main():
    """Fun√ß√£o principal para executar as an√°lises"""
    print("üåê AN√ÅLISE DE GRAFO SOCIAL - GITHUB MINING")
    print("=" * 50)
    
    # Criar inst√¢ncia do analisador
    graph = SocialGraph()
    
    # Carregar dados
    if not graph.load_data():
        print("‚ùå Falha ao carregar dados. Verifique se os arquivos CSV existem.")
        return
    
    # Construir grafo
    graph.build_graph()
    
    if len(graph.nodes) == 0:
        print("‚ùå Nenhum usu√°rio encontrado nos dados.")
        return
    
    try:
        # Executar an√°lises
        print("\n" + "="*50)
        print("üîç EXECUTANDO AN√ÅLISES")
        print("="*50)
        
        # 1. Usu√°rios mais influentes
        top_users = graph.get_most_influential_users(5)
        
        # 2. Usu√°rio mais fragmentador
        fragmenting_user = graph.find_most_fragmenting_user()
        
        # 3. Grupos naturais
        communities = graph.find_natural_groups()
        
        # 4. N√≠vel de conectividade
        connectivity = graph.calculate_community_connectivity()
        
        # 5. Exemplo de usu√°rios pr√≥ximos (usar o primeiro usu√°rio mais influente)
        if top_users:
            example_user = top_users[0][0]
            closest = graph.find_closest_users(example_user, 5)
            non_interacting = graph.find_non_interacting_close_users(example_user, 5)
        
        # Gerar relat√≥rio
        graph.generate_summary_report()
        
        print("\n" + "="*50)
        print("‚úÖ AN√ÅLISES CONCLU√çDAS!")
        print("="*50)
        print("üí° Verifique o arquivo de relat√≥rio gerado para detalhes completos.")
        
    except Exception as e:
        print(f"‚ùå Erro durante as an√°lises: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()